<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Blogs on Hao Lab @ UCSD</title>
    <link>//localhost:1313/blogs/</link>
    <description>Recent content in Blogs on Hao Lab @ UCSD</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 21 Feb 2024 12:00:00 -0800</lastBuildDate>
    <atom:link href="//localhost:1313/blogs/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Consistency Large Language Models: A Family of Efficient Parallel Decoders</title>
      <link>//localhost:1313/blogs/cllm/</link>
      <pubDate>Wed, 21 Feb 2024 12:00:00 -0800</pubDate>
      <guid>//localhost:1313/blogs/cllm/</guid>
      <description>TL;DR: In this blog, we introduce consistency large language models (CLLMs), a new family of models developed with our proposed techniques to reduce inference latency by efficiently decoding $n$ tokens in parallel. This decoding method is called Jacobi decoding, which improves inference efficiency by breaking the sequential nature of conventional auto-regressive (AR) decoding. CLLMs are trained with the objective of performing efficient Jacobi decoding by mapping any randomly initialized $n$-token sequence to a correctly predicted sequence in as few steps as possible.</description>
    </item>
    <item>
      <title>Break the Sequential Dependency of LLM Inference Using Lookahead Decoding</title>
      <link>//localhost:1313/blogs/lookahead_decoding/</link>
      <pubDate>Tue, 21 Nov 2023 12:00:00 -0800</pubDate>
      <guid>//localhost:1313/blogs/lookahead_decoding/</guid>
      <description>TL;DR: We introduce lookahead decoding, a new, exact, and parallel decoding algorithm to accelerate LLM inference. Lookahead decoding breaks the sequential dependency in autoregressive decoding by concurrently extracting and verifying n-grams directly with the LLM, utilizing the Jacobi iteration method. Lookahead decoding functions without the need for a draft model or a data store. It linearly decreases the number of decoding steps directly correlating with the log(FLOPs) used per decoding step.</description>
    </item>
  </channel>
</rss>
