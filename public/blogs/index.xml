<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Blogs on Hao AI Lab @ UCSD</title>
    <link>//localhost:1313/blogs/</link>
    <description>Recent content in Blogs on Hao AI Lab @ UCSD</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 21 Feb 2024 12:00:00 -0800</lastBuildDate>
    <atom:link href="//localhost:1313/blogs/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Consistency Large Language Models: A Family of Efficient Parallel Decoders</title>
      <link>//localhost:1313/blogs/cllm/</link>
      <pubDate>Wed, 21 Feb 2024 12:00:00 -0800</pubDate>
      <guid>//localhost:1313/blogs/cllm/</guid>
      <description>TL;DR: LLMs have been traditionally regarded as sequential decoders, decoding one token after another. In this blog, we show LLMs can be taught to operate as efficient parallel decoders. We introduce Consistency Large Language Models (CLLMs), a new family of parallel decoders capable of reducing inference latency by efficiently decoding a $n$-token sequence in parallel. Our research shows this process &amp;ndash; mimicking human cognitive process of forming complete sentences in mind before articulating word by word &amp;ndash; can be effectively learned by simply finetuning pretrained LLMs.</description>
    </item>
  </channel>
</rss>
